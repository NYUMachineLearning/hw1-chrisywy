---
title: "HW1 Machine Learning"
author: "Wuyue Yu"
date: "9/18/2019"
output: html_document
---

```{r}
data(iris)
summary(iris)
```

```{r load, include=FALSE}
library(ggplot2)
library(tidyverse)
library(ggfortify)
library(fastICA)
library(cluster)
```

```{r}
## 0.Subset the Iris dataset to only include `Sepal.Length`, `Sepal.Width`, `Petal.Length`, and `Petal.Width`.
iris_species <- iris$Species
iris_data <- iris[,c(1:4)]
```

```{r}
## 1. Write out the Kmeans algorithm by hand, and run two iterations of it. 
summary(iris_data)

dis.Euclid <- function(points1, points2) {
  distanceMatrix <- matrix(NA, nrow=dim(points1)[1], ncol=dim(points2)[1])
  for(i in 1:nrow(points2)) {
    distanceMatrix[,i] <- sqrt(rowSums(t(t(points1)-points2[i,])^2))
  }
  distanceMatrix
}

# Choose k = 3
# Step 1. Assign a number,  1 to 3, to each observation
set.seed(123)
randvar <- sample(c('1','2','3'),size = nrow(iris_data),replace=TRUE)
iris_kc0 <- cbind(iris_data,randvar)

# Step 2a. Compute the centroid of each cluster
cluster0.1 <- subset(iris_kc0, iris_kc0$randvar == '1')
cluster0.2 <- subset(iris_kc0, iris_kc0$randvar == '2')
cluster0.3 <- subset(iris_kc0, iris_kc0$randvar == '3')

centroids0 <- rbind(colMeans(cluster0.1[,1:4]),colMeans(cluster0.2[,1:4]),colMeans(cluster0.3[,1:4]))

# Step 2b. Assign each observation to closest centroid

dist1 <- dis.Euclid(iris_data,centroids0)
c.assigned1 <- rep(NA,150)
for (i in 1:150){
  c.assigned1[i] <- which.min(dist1[i,])
}
iris_kc1 <- cbind(iris_data,c.assigned1)

# Step 3a. Compute the new centroid of each cluster
cluster1.1 <- subset(iris_kc1, iris_kc1$c.assigned1 == '1')
cluster1.2 <- subset(iris_kc1, iris_kc1$c.assigned1 == '2')
cluster1.3 <- subset(iris_kc1, iris_kc1$c.assigned1 == '3')
centroids1 <- rbind(colMeans(cluster1.1[,1:4]),colMeans(cluster1.2[,1:4]),colMeans(cluster1.3[,1:4]))

# Step 3b. Assign each observation to closest centroid

dist2 <- dis.Euclid(iris_data,centroids1)
c.assigned2 <- rep(NA,150)
for (i in 1:150){
  c.assigned2[i] <- which.min(dist2[i,])
}
iris_kc2 <- cbind(iris_data,c.assigned2)
```

```{r}
## 2. Run PCA on the Iris dataset. Plot a scatter plot of PC1 vs PC2 and include the percent variance those PCs describe.
iris_data_pca <- data.matrix(iris_data)
iris_pca <- prcomp(iris_data_pca)
autoplot(iris_pca, data = iris, colour = 'Species',loadings = TRUE, loadings.colour = 'purple', loadings.label = TRUE, loadings.label.size = 3)
```

```{r}
## 3. Run ICA on the Iris dataset. Plot the independent components as a heatmap.
iris_ica <- fastICA(iris_data, 4, alg.typ = "parallel", fun = "logcosh", alpha = 1,
        method = "R", row.norm = FALSE, maxit = 200,
        tol = 0.0001, verbose = TRUE)
iris_ica$X
heatmap(iris_ica$S)
```

```{r}
## 4.Use Kmeans to cluster the Iris data. 
#### 4.1 Use the silhouette function in the cluster package to find the optimal number of clusters for kmeans for the iris dataset. 
####     Then cluster using kmeans clustering. Does the data cluster by species? 

avg_sil <- function(k) {
  km.res <- kmeans(iris_data, centers = k, nstart = 25)
  ss <- silhouette(km.res$cluster, dist(iris_data))
  mean(ss[, 3])
}

k.values <- 2:15

avg_sil_values <- map_dbl(k.values, avg_sil)

plot(k.values, avg_sil_values,
     type = "b", pch = 19, frame = FALSE, 
     xlab = "Number of clusters K",
     ylab = "Average Silhouettes",
     xlim = c(1,15), ylim = c(0.3,0.7))

# Choose k=2
set.seed(151)
iris_kmeans <- kmeans(iris_data, 2, nstart=25)
iris_kmeans$cluster
# No. It doesn't cluster by species.

#### Using this clustering, color the PCA plot according to the clusters.
iris_km <- as.factor(iris_kmeans$cluster)
iris_kmdata <- cbind(iris,iris_km)
autoplot(iris_pca, data = iris_kmdata, colour = 'iris_km')
```

```{r}
## 5.Use hierarchical clustering to cluster the Iris data.
#### * Try two different linkage types, and two different distance metrics. 

# Linkage Type: Average, Distance Metric: Euclidean 
hierarchical_dist_1 <- dist(iris_data, method = "euclidean")
tree_1 <- hclust(hierarchical_dist_1, method="average")
plot(tree_1)

# Linkage Type: Centroid, Distance Metric: Euclidean
hierarchical_dist_2 <- dist(iris_data, method = "euclidean")
tree_2 <- hclust(hierarchical_dist_1, method="centroid")
plot(tree_2)

# Linkage Type: Average, Distance Metric: City Block
hierarchical_dist_3 <- dist(iris_data, method = "manhattan")
tree_3 <- hclust(hierarchical_dist_3, method = "average")
plot(tree_3)

#### For one linkage type and one distance metric, try two different cut points
# Linkage Type: Average, Distance Metric: Euclidean, cut points k = 2 and k = 3 
plot(tree_1)
tree_1_k2 <- cutree(tree_1, k = 2)
rect.hclust(tree_1, k = 2, h = NULL)

plot(tree_1)
rect.hclust(tree_1, k = 3, h = NULL)
tree_1_k3 <- cutree(tree_1, k = 3)

# Using this clustering, color the PCA plot according to the clusters. (6  plots in total)
iris_hc1 <- as.factor(tree_1_k3)
iris_hclustdata1 <- cbind(iris,iris_hc1)
autoplot(iris_pca, data = iris_hclustdata1, colour = 'iris_hc1')

iris_hc2 <- as.factor(tree_1_k2)
iris_hclustdata2 <- cbind(iris,iris_hc2)
autoplot(iris_pca, data = iris_hclustdata2, colour = 'iris_hc2')

## References: https://stackoverflow.com/questions/27082378/how-to-compute-distances-between-centroids-and-data-matrix-for-kmeans-algorithm/27088515
##             https://uc-r.github.io/kmeans_clustering#silo
```
